\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Increasing the Effectiveness of Active Learning:\\ 
	\LARGE{Introducing Artificial Data Generation in Active Learning for Land Use/Land Cover Classification}}
\author{
	Joao Fonseca\(^{1}\), Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\) 
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{
	a4paper,
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
	TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
\end{abstract}

\section{Introduction}~\label{sec:introduction}

The technological development of air and space borne sensors, as well as the increasing number of
remote sensing missions have allowed the continuous collection of large amounts of high quality
remotely sensed data. This data is often composed of multi and hyper spectral satellite imagery,
essential for numerous applications, such as Land Use/Land Cover (LULC) change detection, ecosystem
management~\cite{Nagai2020}, agricultural management~\cite{Huang2018}, water resource
management~\cite{Wang2018}, forest management, and urban monitoring~\cite{Khatami2016}. However,
updating a LULC map is still a challenging task~\cite{Gavade2019, Wulder2018}. They can be updated
using either one of the following strategies:

\begin{enumerate}
    \item Photo-interpreted. Consists of evaluating a patch's LULC class based on orthophoto and
        satellite image interpretation~\cite{costa2020introducing}. This method guarantees a decent
        level of accuracy, as it is dependent on the interpreter's expertise and human
        error. Typically, it is an expensive, time-consuming task that requires the expertise of a
        photo-interpreter. This task is also frequently applied to obtain ground-truth labels for
        training and/or validating Machine Learning (ML) algorithms for related
        tasks~\cite{vermote2020remote, COSTANTINO2020}. 
    \item Automated mapping. It is based on the usage of a ML method or a combination of methods in
        order to obtain an updated LULC map. The development of a reliable automated method is still
        a challenge among the ML and remote sensing community, since the efficacy of existing
        methods vary across applications and geographical areas~\cite{Gavade2019}. Typically, this
        method requires the existence of ground-truth data, which is frequently outdated or
        nonexistent for the required time frame~\cite{Nagai2020}. On the other hand, employing a ML
        method provides readily available and relatively inexpensive LULC maps. The increasing
        quality of state-of-the-art classification methods have motivated the application and
        adaptation of these methods in this domain~\cite{Maxwell2018}.
    \item Hybrid approaches. They employ photo-interpreted data to augment the training dataset and
        improve the quality of automated mapping~\cite{Ruzicka2020}. It attempts to accelerate the
        photo-interpretation process by selecting a smaller sample of the study area to be
        interpreted. The goal is to minimize the inaccuracies found in the LULC map by
        supplying high-quality ground-truth data to the automated method. The final
        (photo-interpreted) dataset consists of only the most informative samples, i.e., patches
        that are typically difficult to classify for a traditional automated mapping
        method~\cite{Liu2020}. 
\end{enumerate}

The latter method is best know as Active Learning (AL). It is especially useful whenever there is an
absence of ground-truth data and/or the mapping region does not contain updated LULC
maps~\cite{Su2020}. In a context of limited sample-collection budget, the collection of the most
informative samples capable of optimally increasing the classification accuracy of a LULC map is of
particular interest~\cite{Su2020}. AL attempts to minimize the human-computer interaction involved
in photo-interpretation by selecting the data points to include into the classification process.
These data points are selected based on an uncertainty measure and represent the points close to the
decision borders. Afterwards, they are passed on for photo-interpretation and added to the training
dataset, while the points with the lowest uncertainty values are ignored for photo-interpretation
and classification. This process is iterated until a convergence criterion is
reached~\cite{Pasolli2016}. 

The relevant work developed within AL is described in detail in Section~\ref{sec:al-sota}.
The research attempts to address some of the challenges found in AL, mainly inherited from
automated and photo-interpreted mapping: mapping inaccuracies and time consuming
human-computer interactions. 
Mapping inaccuracies have different sources:

\begin{enumerate}
    \item Human error. The involvement of photo-interpreters in the data labeling step carries an
        additional risk to the creation of LULC patches. The minimum mapping unit being considered,
        as well as the quality of the orthophotos and satellite images being used, are some of the
        factors that may lead to the overlooking of small-area LULC patches and label-noisy training
        data~\cite{Pelletier2017}.
    \item High-dimensional datasets. The amount of bands (i.e., features) present in multi and hyper
        spectral images introduce an increased level of complexity in the classification
        step~\cite{Stromann2020}. These datasets are often prone to the Hughes phenomenon, also
        known as the curse of dimensionality. 
    \item Class separability. Producing an LULC map considering classes with similar spectral
        signatures makes them difficult to separate~\cite{Alonso-Sarria2019}. A lower pixel
        resolution of the satellite images may also imply mixed-class pixels, which may lead to
        both lower class separability as well as higher risk of human error.
    \item Existence of rare land cover classes. The varying morphologies of different geographical
        regions naturally implies an uneven distribution of land cover classes~\cite{Feng2018}. This
        is particularly relevant in the context of AL:\@ the data selection method is based on a
        given uncertainty measure over data points whose class label is unknown. Consequently, AL's
        iterative process of data selection may disregard wrongly classified land cover areas
        belonging to a minority class.
\end{enumerate}

Research developed in the field of Active Learning typically focus on the reduction of human error
by minimizing the human interaction with the process through the development of more efficient
choosers and selection criteria within the generally accepted AL framework.  Concurrently, the
problem of rare land cover classes is rarely addressed. This is a frequent problem in the ML
community, known as the Imbalanced Learning problem.  This problem exists whenever there is an
uneven between-class distribution in the dataset~\cite{Chawla2004}. Specifically, most classifiers
are designed to optimize metrics such as overall accuracy, which are designed to work primarily with
balanced datasets. Consequently, these metrics tend to introduce a bias towards the majority class
by attributing an importance to each class proportional to its relative
frequency~\cite{Maxwell2018}. As an example, such a classifier could achieve an overall accuracy of
99\% on a binary dataset where the minority class represents 1\% of the overall dataset and still be
deemed useless. A number of methods have been developed to deal with this problem. They can be
categorized into three different types of approaches~\cite{Fernandez2013,Kaur2019}. Cost-sensitive
solutions perform changes to the cost matrix in the learning phase. Algorithmic level solutions
modify specific classifiers to reinforce learning on minority classes. Resampling solutions modify
the dataset by removing majority samples and/or generating artificial minority samples. The latter
is independent from the context and can be used alongside any classifier. We will focus on
artificial data generation techniques, presented in Section~\ref{sec:ovs-sota}.

In this paper, we propose a novel AL framework to address two limitations commonly found in the
literature: minimize human-computer interaction and reduce the class imbalance bias. This is done
with the introduction of an additional component in the iterative AL procedure (the generator), used
to generate artificial data to both balance and augment the training dataset. The introduction of
this component is expected to reduce the number of iterations required until convergence of the
predictor's quality.

This paper is organized as follows: Section~\ref{sec:introduction} exposes the problem and its
context, Sections~\ref{sec:al-sota} and~\ref{sec:ovs-sota} describe the state of the art in AL and
Oversampling techniques, Section~\ref{sec:proposed-method} exposes the proposed method,
Section~\ref{sec:methodology} covers the datasets, evaluation metrics, ML classifiers and
experimental procedure, Section~\ref{sec:results} presents the results and statistical analyses and
Section~\ref{sec:conclusion} reports the conclusions drawn from our findings.

\section{Active Learning Approaches}~\label{sec:al-sota}

AL is used as the general definition of frameworks aiming to train a learning system in multiple
steps, where a set of new data points are chosen and added to the training dataset each
time~\cite{Ruzicka2020}. Typically, an AL framework is composed of 10 elements, out of which 4 are
datasets, 2 are queries or estimations regarding the target class labels and 4 are components
responsible for performing the tasks involved in AL~\cite{Sverchkov2017,Su2020,Ruzicka2020}:

\begin{enumerate}
    \item Data source. In the context of LULC classification, the data source is usually a
        hyper/multi-spectral image, a Synthetic-aperture radar (SAR) image, or a composite image.
    \item Unlabeled dataset. Consists of a sample of the original data source. It is used in
        combination with the chooser and the selection criterion to retrieve uncertainty estimates
        on each iteration.
    \item Initial training sample. It is a small sample of the unlabeled dataset, used to initiate
        the first AL iteration. The size of the initial training sample normally varies between no
        observations at all and 10\%~\cite{Li2013}.
    \item Augmented training dataset. This dataset is the concatenation of the labeled initial
        training sample along with the datasets labeled by the oracle in past iterations.
    \item Uncertainty map. The dataset containing the highest uncertainty points/patches to be
        labeled by the oracle.
    \item Oracle. An external entity to which the uncertainty map is presented to. The oracle is
        responsible for annotating unlabeled samples to be added to the augmented dataset. In remote
        sensing, the oracle is typically a photo-interpreter, as is the case in~\cite{li2020}. Some
        of the research also refers to the oracle as the \textit{supervisor}~\cite{Su2020,
        Shrivastava2021}.
    \item Chooser. Produces the class probabilities for each unlabeled sample. This is a classifier
        trained using the augmented dataset. It is used to estimate the class probabilities for each
        sample over the unlabeled dataset.
    \item Selection criterion. It quantifies the chooser's uncertainty level for each sample
        belonging to the unlabeled dataset. It is typically based on the class probabilities
        assigned by the chooser. In some situations, the chooser and the selection criterion are
        grouped together under the concept \textit{acquisition function}~\cite{Ruzicka2020} or
        \textit{query function}~\cite{Su2020}. Some of the literature refers to the selection
        criterion by using the concept \textit{sampling scheme}~\cite{Liu2020}.
    \item Predictor. The classifier used to infer the land cover classes for the final output map.
        Once a stopping criterion is met, the classifier is trained using the augmented dataset and
        the LULC classes are inferred from the data source.
    \item Prediction output. In the context of LULC classification, the prediction output is the
        estimated LULC map raster.
\end{enumerate}

Figure~\ref{fig:al_typical} schematizes the steps involved in a complete AL iteration. For a better
context within the remote sensing domain, the prediction output is identified as the LULC map. This
framework starts by collecting unlabeled data from the original data source. It is used to generate
a random initial training sample and is labeled by the oracle. In practical applications, the oracle
is frequently a group of photo-interpreters~\cite{Kottke2017}. The chooser is trained on the
resulting dataset and is used to predict the class probabilities on the unlabeled dataset. They are
fed into a selection criterion to estimate the prediction's uncertainty, out of which the samples
with the highest uncertainty will be selected. This calculation is motivated by the absence of
labels in the uncertainty dataset. Therefore, it is impossible to estimate the prediction's accuracy
in a real case scenario. The iteration is completed when the selected points are tagged by the
oracle and added to the training dataset (i.e., the augmented dataset). 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.85\linewidth]{../analysis/al_typical}
	\caption{Typical AL framework.
    }~\label{fig:al_typical}
\end{figure}

A common challenge found in AL tasks is ensuring the consistency of AL over different
initializations~\cite{Kottke2017}. There two factors involved in this phenomenon. On the one hand,
the implementation of the same method over different initializations may result in significantly
different accuracy curves. On the other hand, the lack of a robust selection criterion and/or
chooser may also result in results' inconsistency across initializations. This phenomenon was
observed and documented in a LULC classification context in~\cite{tuia2011using}.

Selecting an efficient selection criterion is particularly important to find the samples closest
to the decision border (i.e., samples difficult to classify)~\cite{Shrivastava2021}. Therefore, most
of AL related studies focus on the design of the query/acquisition function~\cite{Su2020}.

\subsection{Non-informed selection criteria}

Only one non-informed selection criterion was found. Random sampling selects unlabeled samples
without considering any external information produced by the chooser. Since the method for selecting
the unlabeled samples is random, this method disregards the usage of a chooser and is comparatively
worse than any other selection criterion. Although, random sampling is still a powerful baseline
method~\cite{Cawley2011}. Generally, different AL initializations return high performance
variability~\cite{Kottke2017}. When this happens, the analysis of the mean performances over multiple
repetitions is not of interest. Instead, it is preferable to do pairwise comparison of different
methods along with their corresponding variances. 

\subsection{Ensemble-based selection criteria}

Ensemble disagreement is based on the class predictions of a set of classifiers. The disagreement
between all the predictions for a given observation is a common measure for uncertainty, although
computationally inefficient~\cite{Ruzicka2020,Pasolli2016}. This method was implemented successfully
for complex applications like deep active learning~\cite{Ruzicka2020}.

Multiview~\cite{Muslea2006} consists on the training of multiple independent classifiers using
different views, which correspond to the selection of subsets of features or observations in the
dataset. Therefore, it can be seen as a bootstrap aggregation (bagging) ensemble disagreement
method. The set of classifications over a single observation is used to calculate the maximum
disagreement metric, given by the number of votes assigned to the most frequent
class~\cite{Shrivastava2021}. A lower value for this metric means a higher classification
uncertainty. Multiview-based maximum disagreement has been successfully applied to hyper-spectral
image classification in~\cite{Di2012} and~\cite{Zhou2014}.
% check whether the definition is 100% correct.

An adapted disagreement criterion for an ensemble of $k$-nearest neighbors has been proposed
in~\cite{Pasolli2016}. This method employs a $k$-nearest neighbors classifier and computes an
instance's classification uncertainty based on the neighbors' class frequency using the maximum
disagreement metric over varying values for $k$. As a result, this method is comparable to computing
the dominant class' score over a weighted $k$-nearest neighbors classifier. This method was also
used on a multimetric active learning framework~\cite{Zhang2016}.

Another relevant ensemble-based selection criterion is the binary random forest-based query
model~\cite{Su2020}. This method employs a one-versus-one ensemble method to demonstrate an
efficient data selection method using the estimated probability of each binary random forest and
determining the classification uncertainty based on the probabilities closest to 0.5 (i.e., the
least separable pair of classes are used to determine the uncertainty value). Although, this study
fails to compare the proposed method with other benchmark methods, such as random sampling.

\subsection{Entropy-based criteria}

A number of contributions have focused on entropy-based querying. The application of entropy is
common among active deep learning applications~\cite{Aghdam2019}, where the training of an ensemble
of classifiers is often too expensive. The measure of entropy is formulated as follows:

\begin{equation}\label{eq:entropy}
    H(x_i) = \sum_{\omega=1}^{N_i}{p(y_{i}^{*}=\omega|x_i)}\log_2[p(y_{i}^{*}=\omega|x_i)]
\end{equation}

The measurement of entropy $H$ is based on the observed probability $p(y_{i}^{*}=\omega|x_i)$ of
obtaining class $\omega$ as the predicted class label $y_{i}^{*}$, where $N_i$ is the number classes
predicted for observation $x_i$.

Entropy query-by-bagging (EQB), also defined as maximum entropy~\cite{Liu2020}, is an ensemble
approach of the entropy selection criterion, originally proposed in~\cite{Tuia2009}. This strategy
uses the set of predictions produced by the ensemble classifier to calculate those many entropy
measurements. The estimated uncertainty measure for one sample is given by the maximum entropy
within that set. EQB was observed to be an efficient selection criterion.
Specifically,~\cite{Shrivastava2021} applied EQB on hyper-spectral remote sensing imagery using
Support Vector Machines (SVM) and Extreme Learning Machines (ELM) as choosers, achieving optimal
results when combining EQB with ELM\@. Another study successfully implemented this method on an
active deep learning application~\cite{Liu2020}. Another study improved over this method with a
normalized EQB selection criterion~\cite{Copa2010}.

\subsection{Other relevant criteria}

Margin Sampling is a SVM-specific criterion, based on the distance of a given point to the SVM's
decision boundary~\cite{Shrivastava2021}. This method is less popular than the remaining methods
because it is limited to one type of chooser (SVMs). One extension of this method is the multiclass
level uncertainty~\cite{Shrivastava2021}, calculated by subtracting the observation's distance to
the decision boundaries of the two most probable classes~\cite{Demir2011}.

The Mutual Information-based (MI) criterion selects the new training samples by maximizing the
mutual information between the classifier and class labels in order to select samples from regions
that are difficult to classify. Although this method is commonly used, it is frequently outperformed
by the breaking ties selection criterion~\cite{Li2011,Liu2018}.

The breaking ties (BT) selection criterion was originally introduced in~\cite{Luo2003}. It is
formulated as follows:

\begin{equation}\label{eq:breaking_ties}
    BT(x_i) = \arg \min_{x_i, i \in S_u}\{ \max_{\omega \in N}{p(y_{i}^{*}=\omega|x_i)} -
    \max_{\omega \in N\setminus\{\omega^{+}\}}{p(y_{i}^{*}=\omega|x_i)}\}
\end{equation}

Which is the subtraction of the probabilities of the two most likely classes. Another
related method is Modified Breaking Ties scheme (MBT), which aims at finding the samples containing
the largest probabilities for the dominant class~\cite{Liu2018,Li2013a}

The last type of selection criteria identified is the loss prediction method~\cite{Yoo2019}. This
method replaces the selection criterion with a second predictor whose goal is to estimate the
chooser's loss for a given prediction. This allows the new classifier to estimate the prediction
loss on unlabeled observations and select the ones with the highest predicted loss.

Some of the literature fail to specify the strategy employed, although inferring it is generally
intuitive. For example,~\cite{Ertekin2007} successfully used AL to address the imbalanced learning
problem. They employed an ensemble of SVMs as the chooser and predictor to employ an ensemble-based
selection criterion. All of the research found related to this topic focused on the improvement of AL
through modifications on the selection criterion, chooser or predictor. None of these publications
proposed significant variations to the typical AL framework.

\section{Artificial Data Generation Approaches}~\label{sec:ovs-sota}

The generation of artificial data is a common approach to address imbalanced learning
tasks~\cite{Kaur2019}, as well as improving the effectiveness of supervised learning
tasks~\cite{DeVries2017}. In recent years some sophisticated data generation approaches were found.
Although, the scope of this work is to propose the integration of a generator within the AL
framework. Due to the complexity and computational cost of network-based approaches (e.g.,
Generative Adversarial Networks), we will only focus on heuristic data generation approaches.

Heuristic data resampling methods employ local and/or global information to generate new, relevant,
non-duplicated instances. These methods are most commonly used to populate minority classes and
balance the between-class distribution of a dataset. The Synthetic Minority Oversampling Technique
(SMOTE)~\cite{Chawla2002} was the first heuristic oversampling algorithm to be proposed. The
simplicity and effectiveness of this method contributes to its prevailing popularity. It generates a
new instance $\overrightarrow{z}$ through a linear interpolation of a randomly selected
minority-class observation $\overrightarrow{x}$ and one of its randomly selected $k$-nearest
neighbors $\overrightarrow{y}$ such that $\overrightarrow{z} = \alpha\overrightarrow{x} +
(1-\alpha)\overrightarrow{y}$ where $\alpha$ is a random float between 0 and 1, as shown in
Figure~\ref{fig:data_generation}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{../analysis/data_generation}
	\caption{Examples of SMOTE and G-SMOTE generation process.
    }~\label{fig:data_generation}
\end{figure}

The implementation of SMOTE for LULC classification tasks has been found to improve the quality of
the predictors used~\cite{Jozdani2019,Bogner2018}. Despite its popularity, its drawbacks motivated
the development of other oversampling methods~\cite{Douzas2019}:

\begin{enumerate}
    \item Generation of noisy samples due to the selection of $k$-nearest neighbors and initial
        observation. The selection of a sample and/or neighboring sample located inside a majority
        class region may produce artificial samples within that region and amplify noisy data.
        Borderline-SMOTE~\cite{Han2005} is a modification of SMOTE in which only the minority
        examples near the borderline are over-sampled. This method avoids the generation of noisy
        samples by disregarding minority samples located in a majority class region as well as
        samples distant from the decision borders. The Adaptive Synthetic Sampling approach
        (ADASYN)~\cite{HaiboHe2008} uses a density distribution ratio to address this limitation and
        focus the artificial data generation on minority class regions that are more difficult to
        classify.
    \item Generation of noisy instances due to the use of observations from two different minority
        class clusters. Choosing a minority sample $\overrightarrow{a}$ and one of its nearest
        neighbors $\overrightarrow{b}$ belonging to a different minority cluster may lead to the
        generation of a sample $\overrightarrow{c}$ located within the two classes, as shown in
        Figure~\ref{fig:data_generation}. K-means SMOTE~\cite{Douzas2018} and Self-Organizing map
        oversampling (SOMO)~\cite{Douzas2017} reduce this effect by oversampling minority class
        samples within the same clusters.
    \item Generation of nearly duplicated instances. The linear interpolation of parent samples that
        are close to each other produces an artificial sample with similar properties as its
        parents. Geometric SMOTE (G-SMOTE)~\cite{Douzas2019} introduces a modification of the SMOTE
        algorithm in the data generation mechanism to produce artificial samples with higher
        variability.
\end{enumerate}

The G-SMOTE algorithm is introduced as a generalization of the vanilla SMOTE\@. Instead of
generating artificial data as a linear combination of the parent samples, it is done within a
deformed, truncated hyper-spheroid. G-SMOTE generates an artificial sample $\overrightarrow{r}$
within a hyper-spheroid $H$, formed by selecting a minority sample $\overrightarrow{p}$ and one of
its nearest neighbors $\overrightarrow{q}$, as shown in Figure~\ref{fig:data_generation}. The
truncation and deformation parameters define the shape of the spheroid's geometry. The method also
modifies the selection strategy for the $k$-nearest neighbors, accepting the generation of
artificial samples using observations from different classes. G-SMOTE has shown superior performance
when compared with other oversampling methods for LULC classification tasks, regardless of the
classifier used~\cite{Douzas2019class}.

\section{Proposed method}~\label{sec:proposed-method}

Within the literature identified, most of the work developed in the AL domain revolved around
improving the quality of the chooser, predictor and/or selection criterion. Although these methods
allow earlier convergence of the AL iterative process, the impact of these methods are only observed
between iterations. Consequently, none of these contributions focused on the definition of decision
borders within iterations. The method proposed in this paper modifies the AL framework by
introducing an artificial data generation step within AL's iterative process. We define this
component as the generator, as defined in Figure~\ref{fig:al_new}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=.85\linewidth]{../analysis/al_new}
	\caption{Proposed AL framework.
    }~\label{fig:al_new}
\end{figure}

This method focuses on the capacity of artificial data to introduce more data variability into the
augmented dataset and facilitate the chooser's training phase with a more consistent definition of
the decision boundaries at each iteration. The artificial data is only used to train the classifiers
involved in the process (chooser and predictor) and is discarded every time it is used to train the
chooser. The remaining steps in the AL framework remain unchanged. This method is addressed towards
the limitations found in the previous sections: 

\begin{enumerate}
    \item The lack of consistency of the chooser's performance over different initializations should
        be mitigated with the introduction of artificial data early on in the iterative process.
    \item The convergence of the predictor's performance should be anticipated with the clearer
        definition of the decision boundaries across iterations.
    \item Annotation cost is expected to reduce as the need for labeled observations reduces along
        with the early convergence of the classification performance.
    \item The class imbalance bias observed in typical classification tasks, as well as in AL is
        mitigated by balancing the class frequencies at each iteration.
\end{enumerate}

Although the performance of this method is shown within a LULC classification
context, the proposed framework is completely domain agnostic. The high
dimensionality of remotely sensed imagery make its classification particularly
challenging when the availability of labeled data is scarce and/or comes at a
high cost, being subjected to the curse of dimensionality. Consequently, it is a
relevant and appropriate domain to test this method.

\section{Methodology}~\label{sec:methodology}

In this section we describe the datasets, evaluation metrics, oversamplers,
classifiers, software used and the procedure developed. We demonstrate the
proposed method's efficiency over 7 datasets, sampled from well-known benchmark
remote sensing landscapes frequently found in the literature. The datasets and
sampling strategy are described in Subsection~\ref{sec:datasets}. On each of
these datasets, we implement 3 different classifiers over the entire training
set to estimate the optimal classification performance, the traditional AL
framework as the baseline reference and the proposed method using two different
generators, described in Subsection~\ref{sec:machine_learning_algorithms}. The
metrics used to estimate the performance of these algorithms are described in
Subsection~\ref{sec:evaluation_metrics}. Finally, the experimental procedure is
described in Subsection~\ref{sec:experimental_procedure}. 

Our methodology focuses on three objectives: (1) Comparison of optimal
classification performance among active learners and traditional supervised
learning, (2) Comparison of classification convergence efficiency across AL
frameworks and (3) Comparison of classification convergence across selection
criteria and AL frameworks.

\subsection{Datasets}~\label{sec:datasets}

The datasets used were extracted from publicly available hyperspectral scenes.
Additionally, all datasets were collected using the same sampling procedure. The
description of the hyperspectral scenes used in this study is provided in
Table~\ref{tab:rs_scene_description}.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=semicolon, header=true,
	columns={Dataset,Sensor,Location,Dimension,Bands,Res. (m),Classes}, string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:rs_scene_description}
				Description of the hyperspectral scenes used for this experiment.}}
]{../analysis/rs_scene_description.csv}

The Indian Pines scene~\cite{Baumgardner2015} is composed of agriculture fields
in approximately two thirds of its coverage, low density buildup areas and
natural perennial vegetation in the remainder of its area (see
Figure~\ref{fig:indian_pines}). The Pavia Centre and University scenes are
hyperspectral, high-resolution images containing ground truth data composed of
urban-related coverage (see Figures~\ref{fig:pavia_centre}
and~\ref{fig:pavia_university}). The Salinas and Salinas A scenes contain
at-sensor radiance data. As subset of Salinas, the Salinas A scene contains
contains the vegetables fields present in Salinas and the latter is also
composed of bare soils and vineyard fields (see Figures~\ref{fig:salinas}
and~\ref{fig:salinas_a}). The Botswana scene contains ground truth data composed
of seasonal swamps, occasional swamps, and drier woodlands located in the distal
portion of the Delta (see Figure~\ref{fig:botswana}). The Kennedy Space Center
scene contains a ground truth composed of both vegetation and urban-related
coverage (see Figure~\ref{fig:kennedy_space_center})

\begin{figure}[H]
	\centering
	\begin{subfigure}{.24\textwidth}
		\centering
		\captionsetup{skip=12pt}
		\includegraphics[height=1.5\linewidth]{../analysis/indian_pines}
		\subcaption{{\medbreak}}\label{fig:indian_pines}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/pavia_centre}
		\subcaption{{\medbreak}}\label{fig:pavia_centre}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/pavia_university}
		\subcaption{{\medbreak}}\label{fig:pavia_university}
	\end{subfigure}

	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/salinas}
		\subcaption{{\medbreak}}\label{fig:salinas}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/salinas_a}
		\subcaption{{\medbreak}}\label{fig:salinas_a}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/botswana}
		\subcaption{{\medbreak}}\label{fig:botswana}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[height=1.5\linewidth]{../analysis/kennedy_space_center}
		\subcaption{{\medbreak}}\label{fig:kennedy_space_center}
	\end{subfigure}
	\caption{Gray scale visualization of a band (top row) and ground truth (bottom row) of
		each scene used in this study. (a) Indian Pines, (b) Pavia Centre, (c) Pavia
		University, (d) Salinas, (e) Salinas A, (f) Botswana, (g) Kennedy Space Center
    }\label{fig:scenes}
\end{figure}

The sampling strategy is similar to all datasets. The pixels without a ground
truth label are first discarded. All the classes with cardinality lower than 150
are also discarded. This is done to maintain feasible Imbalance Ratios (IR)
across datasets (where $IR = \frac{count(C_{maj})}{count(C_{min})}$). Finally, a
stratified sample of 1500 observations are selected for the experiment. The
resulting datasets are described in Table~\ref{tab:datasets_description}. The
motivation for this strategy is three fold: (1) reduce the datasets to a
manageable size and allow the experimental procedure to be completed within a
feasible time frame, (2) ensure the relative class frequencies in the scenes are
preserved and (3) ensure equivalent analyses across datasets and AL frameworks.
In this context, a fixed number of observations per dataset is especially
important to standardize the AL-related performance metrics.

\pgfplotstabletypeset[
	begin table=\begin{longtable},
		end table=\end{longtable},  col sep=comma, header=true,
	columns={Dataset,Features,Instances,Min. Instances,Maj. Instances,IR, Classes}, string type, every head row/.style={before row=\toprule, after row=\midrule\endhead},
	every last row/.style={after row=\bottomrule \caption{\label{tab:datasets_description}
				Description of the datasets used for this experiment.}}
]{../analysis/datasets_description.csv}

\subsection{Machine Learning Algorithms}~\label{sec:machine_learning_algorithms}

We use two different types of ML algorithms. Data generation algorithms, used to
form the generator, and classification algorithms, used to form the chooser and
predictor. In order to maintain simplicity and a common aproach to most of the
literature in the topic, the classifiers used to play the chooser and predictor
are the same.

Although any method capable of generating artificial data can be used as a
generator, the ones used in this experiment are oversamplers, originally
developed to deal with imbalanced learning problems. Specifically, we chose
SMOTE for its popularity and simplicity. We also chose G-SMOTE as a better
performing generalization of the former method.

Three classification algorithms are used as the chooser and predictor. We use
different types of classifiers to test the framework's performance under varying
situations: neighbors-based, linear and ensemble models. The neighbors-based
classifier chosen was $K$-nearest neighbors (KNN)~\cite{Cover1967}, a logistic
regression (LR)~\cite{Nelder1972} is used as the linear model and a random
forest classifier (RFC)~\cite{Ho1995} was used as the ensemble model.

The acquisition function is completed by testing three different selection
criteria. Random selection is used as a baseline selection criterion, whereas
entropy (see Formula~\ref{eq:entropy}) and breaking ties (see
Formula~\ref{eq:breaking_ties}) are used due to their popularity and classifier
independence. 

\subsection{Evaluation Metrics}~\label{sec:evaluation_metrics}

According to~\cite{Gavade2019}, nearly 80\% of the stallite-based LULC studies
employ the \textit{Overall Accuracy} (OA) and \textit{Kappa coefficient}
performance metrics. However, these metrics are frequently insufficient to
accurate depict the classification performance~\cite{Olofsson2013, Pontius2011}.
Metrics such as Producer's Accuracy (or \textit{Recall}) and User's Accuracy (or
\textit{Precision}) are commonly used. Since they consist of ratios based on
True/False Positives (TP and FP) and Negatives (TN and FN), formulated as
$Precision = \frac{TP}{TP+FP}$ and $Recall = \frac{TP}{TP+FN}$, they provide per
class information regarding the classifier's classification performance.
However, in this experiment, the meaning and number of classes available in each
dataset varies, making these metrics difficult to synthesize.

While OA and Kappa tend to overestimate a classifier's performance on datasets
with high IR, other metrics such as \textit{F-score} and \textit{Geometric mean}
(G-mean) are less sensitive to the data imbalance bias~\cite{Jeni2013,
Kubat1997}. Therefore, employ 3 performance metrics:

\begin{enumerate}
    \item The G-mean scorer is the geometric mean of $Specificity = \frac{TN}{TN
        + FP}$ and \textit{Sensitivity} (also known as
        \textit{Recall})~\cite{Kubat1997}. Both metrics are calculated in a
        multiclass context considering a one-versus-all approach. For multiclass
        problems, the \textit{G-mean} scorer is calculated as its average per
        class values: 
        \begin{equation}\label{eq:gmean}
            \textit{G-mean} = \sqrt{\overline{Sensitivity}_i \times
            \overline{Specificity}_i}
        \end{equation}
    
    \item F-score is the harmonic mean of \textit{Precision} and
        \textit{Recall}. The two metrics are also calculated considering a
        one-versus-all approach. The \textit{F-score} for the multi-class case
        can be calculated using its average per class values~\cite{He2009}:
        \begin{equation}\label{eq:fscore}
            \textit{F-score}=2\frac{\overline{Precision} \times
            \overline{Recall}}{\overline{Precision} + \overline{Recall}}
        \end{equation}

    \item OA consists of the ratio between the number of correctly classified
        observations and the total number of observations. This metric, because of its
        popularity and easy interpretability, is kept for discussion purposes.
        Considering $C$ as the set of classes within a dataset, it is expressed as: 
        \begin{equation}\label{eq:oa}
            \textit{OA} =
            \frac{\sum_{i}^{C}{\text{TP}_{i}}}{\sum_{i}^{C}{(\text{TP}_{i} +
            \text{FP}_{i})}}
        \end{equation}

\end{enumerate}

The comparison of classification convergence across AL frameworks and selection
criteria is done using 3 AL-specific performance metrics. Particularly, we
follow the recommendations found in~\cite{Kottke2017}. Each AL configuration is
evaluated using the \textit{Area Under the Learning Curve} (AULC) performance
metric. It is the sum of the classification performance values of all
iterations. To facilitate the analysis of the results, we fix the range of this
metric between $[0,1]$ by dividing this metric with the total amount of
iterations (i.e., the maximum performance area). The metric \textit{Data
Utilization Rate} (DUR)~\cite{Reitmaier2013} consists of the ratio between the
minimum number of observations necessary to reach a given performance threshold
by an AL strategy and an equivalent strategy using the random selection
criterion. The deficiency score~\cite{Yanik2015} is used to compare the
performance between two active learners. The deficiency of algorithm $A$ with
respect to algorithm $B$ is calculated with the areas between the respective
learning curves and the maximum performance line $MP$:

\begin{equation}
    deficiency = \frac{MP-AULC_A}{2MP-AULC_A-AULC_B} 
\end{equation}

\subsection{Experimental Procedure}~\label{sec:experimental_procedure}

A common practice in methodological evaluations is the implementation of an
offline experiment~\cite{Kagy2019}. It consists of using an existing set of
labeled data as a proxy for the population of unlabeled samples. Because the
dataset is already fully labeled, the oracle's typical annotation process
involved in each iteration is done at zero cost. Each AL and classifier
configuration is tested using a stratified 5-fold cross validation testing
scheme. For each round, the larger partition is split in a stratified fashion to
form a training and validation set (containing 20\% of the original partition).
The validation set is used to evaluate the convergence efficiency of active
learners; the chooser's classification performance metrics and amount of data
points used at each iteration are used to compute the AULC and DUR\@.
Additionally, within the AL iterative process, the classifier with optimal
performance on the validation set is evaluated using the test set. In order to
further reduce the results' variability, this procedure is repeated 3 times with
different seeds and the results of all runs are averaged (i.e., each
configuration is trained and evaluated 15 times). Finally, the maximum
performance lines are calculated using the same approach. In those cases, the
validation set is not used. The experimental procedure is depicted in
Figure~\ref{fig:experiment_pipeline}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/experiment_pipeline}
	\caption{Experimental procedure. The performance metrics are averaged over
    the 5 folds across each of the 3 different initializations of this procedure
    for a given combination of generator, chooser/predictor and selection
    criterion.}~\label{fig:experiment_pipeline}
\end{figure}

To make the convergence metrics comparable across active learners, the
configurations of the different frameworks must be similar. For each dataset,
the number of observations is constant to facilitate the analysis of the same
metrics. 

In most practical AL applications it is assumed that the number of observations
in the initial training sample is too small to perform hyperparameter tuning.
Consequently, in order to ensure realistic results, our experimental procedure
does not include hyperparameter optimization. The predefined hyperparameters are
shown in Table~\ref{tab:grid}. They were set up based on general recommendations
and default settings for the classifiers and generators used.

The AL iterative process is set up with a randomly selected initial training
sample with 15 initial samples. At each iteration, an additional 15 samples are
added to the training set. This process is stopped after 49 iterations, once
50\% of the dataset is added to the augmented dataset.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier & Hyperparameters      & Values             \\
		\midrule
		LR         & maximum iterations   & 10000              \\
		           & solver               & sag                \\
                   & penalty              & None               \\
		KNN        & \# neighbors         & 5                  \\
                   & weights              & uniform            \\
                   & metric               & euclidean          \\
		RF         & maximum tree depth   & None               \\
		           & \# estimators        & 100                \\
                   & criterion            & gini               \\
		\toprule
		Oversampler &                     &                    \\
		\midrule
		SMOTE      & \# neighbors         & 5                  \\
		G-SMOTE    & \# neighbors         & 5                  \\
                   & deformation factor   & 0.5                \\
                   & truncation factor    & 0.5                \\
		\bottomrule
	\end{tabular}
    \caption{\label{tab:grid}Hyper-parameters grid.}
\end{table}

\subsection{Software Implementation}

The experiment was implemented using the Python programming language, along with
the Python libraries
\href{https://scikit-learn.org/stable/}{Scikit-Learn}~\cite{Pedregosa2011},
\href{https://imbalanced-learn.org/en/stable/}{Imbalanced-Learn}~\cite{JMLR:v18:16-365},
\href{https://geometric-smote.readthedocs.io/en/latest/?badge=latest}{Geometric-SMOTE},
\href{https://cluster-over-sampling.readthedocs.io/en/latest/?badge=latest}{Cluster-Over-Sampling}
and
\href{https://research-learn.readthedocs.io/en/latest/?badge=latest}{Research-Learn}
libraries.  All functions, algorithms, experiments and results are provided at
the
\href{https://github.com/AlgoWit/publications/tree/master/remote-sensing/al-generator}{GitHub
repository of the project}.

\section{Results}~\label{sec:results}

% evaluation criteria

% AL with vs without oversampling
% AL with and without random selection 
% AL vs fully labeled dataset

\subsection{Statistical Analysis}

\section{Conclusion}~\label{sec:conclusion}

\bibliography{references}
\bibliographystyle{apalike}

\end{document}
