\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\usepackage{breakcites}
\usepackage[square,numbers]{natbib}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{%
	a4paper,
	left=18mm,
	right=18mm,
	top=18mm,
}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\definecolor{hypecol}{HTML}{0875b7}
\hypersetup{%
    colorlinks,
    linkcolor={hypecol},
    citecolor={hypecol},
    urlcolor={hypecol}
}

\title{%
    Heuristic Data Augmentation Improves the Efficiency of Active Learning
    Methods
}

\author{%
	Joao Fonseca\(^{1*}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de
    Campolide, 1070--312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\begin{document}

\maketitle

\begin{abstract}
    Work in progress (Abstract).
    % - Proposed method outperforms the remaining frameworks
    % - Proposed method is more robust to the choice of domain, classifiers
    %   and performance metrics being chosen
\end{abstract}

\section{Introduction}

Work in Progress (Introduction).

\section{Active Learning Methods}

Work in progress (Active Learning Methods --- Lit. Review).

Reminders:
\begin{itemize}
    \item Discuss maximum performance thresholds.
\end{itemize}

\section{Data Augmentation Methods}

Work in progress (Data Augmentation Methods --- Lit. Review).

\section{Proposed Method}~\label{sec:proposed_method}

Work in progress (Proposed Method)

\section{Methodology}~\label{sec:methodology}

This section describes the different elements included in the experimental
procedure. The datasets used were acquired in open data repositories and its
sources and preprocessing steps are defined in Subsection~\ref{sec:datasets}.
The choice of classifiers used in the experiment are defined in
Subsection~\ref{sec:machine_learning_algorithms}. The metrics chosen to
measure AL performance and overall classification performance are defined in
Subsection~\ref{sec:evaluation_metrics}. The experimental procedure is
described in Subsection~\ref{sec:experimental_procedure}. The implementation
of the experiment and resources used to do so are described in
Subsection~\ref{sec:software_implementation}.

The methodology developed serves 2 purposes: (1) Compare classification
performance once all the AL procedures are completed (\textit{i.e.,} optimal
performance of a classifier trained via iterative data selection) and (2)
Compare the amount of data required to reach specific performance thresholds
(\textit{i.e.,} number of AL iterations required to reach similar the optimal
classification performance).

\subsection{Datasets}~\label{sec:datasets}

The datasets used to test the proposed method are publicly available in open
data repositories. Specifically, they were retrieved from
\href{https://www.openml.org/}{OpenML} and the
\href{https://archive.ics.uci.edu/}{UCI Machine Learning Repository}. The
datasets were chosen considering different domains of application, imbalance
ratios, dimensionality and number of target classes, all of them focused on
classification tasks. The goal is to demonstrate the performance of the
different AL frameworks in various scenarios and domains. The data
preprocessing approach was similar across all datasets.
Table~\ref{tab:datasets_description} describes the key properties of the 10
preprocessed datasets where the experimental procedure was applied. 

\begin{table}[H]
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/datasets_description.csv}
    \caption{\label{tab:datasets_description}
        Description of the datasets collected after data preprocessing. The
        sampling strategy is similar across datasets. Legend: (IR) Imbalance
        Ratio
    }
\end{table}

The data preprocessing pipeline is depicted as a flowchart in
Figure~\ref{fig:data_preprocessing}. The missing values are removed from each
dataset by removing the corresponding observations. This ensures that the
input data in the experiment is kept as close to its original form as
possible. The non-metric features (\textit{i.e.,} binary, categorical and
ordinal variables) were removed since the application of G-SMOTE is limited to
continuous and discrete features. The datasets containing over 2000
observations were downsampled in order to maintain the datasets to a
manageable size. The data sampling procedure preserves the relative class
frequency of the dataset, in order to maintain the Imbalance Ratio (IR)
originally found in each dataset (where $IR =
\frac{count(C_{maj})}{count(C_{\min})}$). The remaining features of each
dataset are scaled to the range of $[-1, 1]$ to ensure a common range across
features.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/data_preprocessing}
    \caption{%
        Data preprocessing pipeline.
    }~\label{fig:data_preprocessing}
\end{figure}

The preprocessed datasets were stored into a SQLite database file and is
available along with the experiment's source code in the GitHub repository of
the project (see Subsection~\ref{sec:software_implementation}).

\subsection{Machine Learning Algorithms}~\label{sec:machine_learning_algorithms}

We used a total of 4 classification algorithms and a heuristic data
augmentation mechanism. The choice of classifiers was based on the popularity
and family of the classifiers (tree-based, nearest neighbors-based,
ensemble-based and linear models). Our proposed method was tested using a
Decision Tree (DT)~\cite{Wu1975}, a K-nearest neighbors classifier
(KNN)~\cite{Cover1967}, a Random Forest Classifier (RF)~\cite{Ho1995} and a
Logistic Regression (LR)~\cite{Nelder1972}. Since the target variables are
multi-class, the LR classifier was implemented using the one-versus-all
approach. The predicted class is assigned to the label with the highest
likelihood.

The oversampler G-SMOTE was used as a data augmentation method. The typical
sampling strategy of oversampling methods is to generate artificial
observations on non-majority classes such that the number of majority class
observations matches those of each non-majority class. We modified this
sampling strategy to generate observations for all classes, as a percentage of
the number of observations in the majority class. In addition, the original
G-SMOTE algorithm was modified to accept data selection probabilities based on
classification uncertainty. These modifications are discussed in
Section~\ref{sec:proposed_method}.

Every AL procedure was tested with different selection criteria: Random
Selection, Entropy and Breaking Ties. The baseline used is the standard AL
procedure. As a benchmark, we add the AL procedure using G-SMOTE as a normal
oversampling method, as proposed in~\cite{Fonseca2021}. Our proposed method
was implemented using G-SMOTE as a data augmentation method to generate
artificial observations for all classes, while still balancing the class
distribution, as described in Section~\ref{sec:proposed_method}. 

\subsection{Evaluation Metrics}~\label{sec:evaluation_metrics}

Considering the imbalanced nature of the datasets used in the experiment,
commonly used performance metrics such as Overall Accuracy (OA), although
being intuitive to interpret, are insufficient quantify a model's
classification performance~\cite{Jeni2013}. The Cohen's Kappa performance
metric, similar to OA, is also biased towards high frequency classes since its
definition is closely related to the OA metric, making its behavior consistent
with OA~\cite{Fatourechi2008}. However, these metrics remain popular choices
for the evaluation of classification performance. Other performance metrics
like $Precision = \frac{TP}{TP+TN}$, $Recall = \frac{TP}{TP+FN}$ or
$Specificity = \frac{TN}{TN + FP}$ are calculated as a function of True/False
Positives (TP and FP) and True/False Negatives (TN and FN) and can be used at
a per-class basis instead. In a multiple dataset with varying amount of target
classes and meanings, comparing the performance of different models using
these metrics becomes impractical.

Based on the recommendations found in~\cite{Jeni2013, Kubat1997}, we used 2
metrics found to be less sensitive to the class imbalance bias, along with OA
as a reference for easier interpretability:

\begin{itemize}
    \item The Geometric-mean scorer (G-mean) consists of the geometric mean of
        Specificity and Recall~\cite{Kubat1997}. Both metrics are calculated
        in a multiclass context considering a one-versus-all approach. For
        multiclass problems, the G-mean scorer is calculated as its average
        per class values: 
        
        \begin{equation*}
            \textit{G-mean} = \sqrt{\overline{Sensitivity} \times
            \overline{Specificity}}
        \end{equation*}

    \item The F-score metric consists of the harmonic mean of Precision and
        Recall. The two metrics are also calculated considering a
        one-versus-all approach. The F-score for the multi-class case
        can be calculated using its average per class values~\cite{Jeni2013}:

        \begin{equation*}
            \textit{F-score}=2\times\frac{\overline{Precision} \times
            \overline{Recall}}{\overline{Precision} + \overline{Recall}}
        \end{equation*}

    \item The OA consists of the number of TP divided by the total amount of
        observations. Considering $c$ as the label for the different classes
        present in a target class, OA is given by the following formula:

        \begin{equation*}
            \textit{OA} = \frac{\sum\limits_{c}{\text{TP}_{c}}}{%
		    	      \sum\limits_{c}{(\text{TP}_{c}+\text{FP}_{c})}}
        \end{equation*}
\end{itemize}

The comparison of the performance of AL frameworks is based on its data
selection and augmentation efficacy. Specifically, an efficient data
selection/generation strategy allows the production of classifiers with high
performance on unseen data while using as least non-artificial training data
as possible. To measure the performance of the different AL setups, we follow
the recommendations found in~\cite{Kottke2017}. The performance of an AL setup
will be compared using two AL-specific performance metrics:

\begin{itemize}

    \item Area Under the Learning Curve (AULC). It is the sum of the
        classification performance over a validation/test set of the
        classifiers trained of all AL iterations. To facilitate the
        interpretability of this metric, the resulting AULC scores are fixed
        within the range $[0, 1]$ by dividing the AULC scores by the total
        amount of iterations (\textit{i.e.}, the maximum performance area).

    \item Data Utilization Rate (DUR)~\cite{Reitmaier2013}. Measures the
        percentage of training data required to reach a given performance
        threshold, as a ratio of the percentage of training data required by
        the baseline framework. This metric is also presented as a percentage
        of the total amount of training data, without making it relative to
        the baseline framework. The DUR metric is measured at 45 different
        performance thresholds, ranging between $[0.10, 1.00]$ at a 0.02 step.

\end{itemize}

\subsection{Experimental Procedure}~\label{sec:experimental_procedure}

The evaluation of different active learners in a live setting is generally
expensive, time-consuming and prone to human error. Instead, a common practice
is to compare them in an offline environment using labeled
datasets~\cite{Kagy2019}. In this scenario, since the dataset is already
labeled, the annotation process is done at zero cost.
Figure~\ref{fig:experimental_procedure} depicts the experiment designed for
one dataset over a single run. 

% TODO:
% - Add percentage of observations in each data subset
% - Mention the maximum percentage of data used by the active learners within
%   the train set
A single run starts with the splitting of a preprocessed dataset in 5
different partitions, stratified according to the class frequencies of the
target variable using the K-fold Cross Validation method. During this run, an
active learner or classifier is trained 5 times using a different partition as
the Test set each time. For each training process, a Validation set is created
and is used to measure the data selection efficiency (\textit{i.e.,} AULC and
DUR using the classification performance metrics, specific to AL). The AL
simulations and the classifiers' training occur within the Train set. Once the
training phase is completed, the Test set classification scores are calculated
using the trained classifiers. For the case of AL, the classifier with the
optimal Validation set score is used to estimate the AL's optimal
classification performance over unseen data.

The process shown in Figure~\ref{fig:experimental_procedure} is repeated over
3 runs using different random seeds over the 10 different datasets collected.
The final scores of each AL configuration and classifier correspond to the
average of the 3 runs and 5-fold Cross Validation estimations (\textit{i.e.,}
the mean score of 15 fits, across 10 datasets).

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\linewidth]{../analysis/experimental_procedure}
    \caption{%
        Experimental procedure flowchart.
    }~\label{fig:experimental_procedure}
\end{figure}

The hyperparameters defined for the AL frameworks, Classifiers and Generators
are shown in Table~\ref{tab:grid}. In the Generators table, we distinguish the
G-SMOTE algorithm working as a normal oversampling method from G-SMOTE-AUGM,
which performs generates additional artificial data on top of the usual
oversampling mechanism. Since the G-SMOTE-AUGM method is intended to be used
with varying parameter values (via within-iteration parameter tuning), the
parameters were defined as a list of various possible values.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Active Learners & Hyperparameters                  &                                \\
		\midrule
		Standard        & \# initial obs.\                  & 1.6\%                          \\
                        & \# additional obs.\ per iteration & 1.6\%                          \\
                        & max.\ iterations + initialization & 50                             \\
                        & evaluation metrics                & G-mean, F-score, OA            \\
                        & selection Strategy                & Random, Entropy, Breaking Ties \\
                        & within-iteration param.\ tuning   & None                           \\
                        & generator                         & None                           \\
                        & classifier                        & DT, LR, KNN, RF                \\
        Oversampling    & generator                         & G-SMOTE                        \\
        Proposed        & generator                         & G-SMOTE-AUGM                   \\
                        & within-iteration param.\ tuning   & Grid Search K-fold CV          \\
		\toprule
		Classifier      &                                  & Values                         \\
		\midrule
        DT              & min.\ samples split              & 2                              \\
                        & criterion                        & gini                           \\
		LR              & maximum iterations               & 100                            \\
                        & multi class                      & One-vs-All                     \\
		                & solver                           & liblinear                      \\
                        & penalty                          & L2 (Ridge)                     \\
		KNN             & \# neighbors                     & 5                              \\
                        & weights                          & uniform                        \\
                        & metric                           & euclidean                      \\
		RF              & min.\ samples split              & 2                              \\
		                & \# estimators                    & 100                            \\
                        & criterion                        & gini                           \\
		\toprule
		Generator       &                                  &                                \\
		\midrule
		G-SMOTE         & \# neighbors                     & 4                              \\
                        & deformation factor               & 0.5                            \\
                        & truncation factor                & 0.5                            \\
		G-SMOTE-AUGM    & \# neighbors                     & 3, 4, 5                        \\
                        & deformation factor               & 0.5                            \\
                        & truncation factor                & 0.5                            \\
                        & augmentation factor              & $[1.1, 2.0]$ at 0.1 step       \\
		\bottomrule
	\end{tabular}
    \caption{\label{tab:grid}
        Hyper-parameter definition for the active learners, classifiers and
        generators used in the experiment.
    }
\end{table}

\subsection{Software Implementation}~\label{sec:software_implementation}

The experiment was implemented using the Python programming language, along
with the Python libraries
\href{https://scikit-learn.org/stable/}{Scikit-Learn}~\cite{Pedregosa2011},
\href{https://imbalanced-learn.org/en/stable/}{Imbalanced-Learn}~\cite{JMLR:v18:16-365},
\href{https://geometric-smote.readthedocs.io/en/latest/?badge=latest}{Geometric-SMOTE}~\cite{Douzas2019},
\href{https://research-learn.readthedocs.io/en/latest/?badge=latest}{Research-Learn}
and
\href{https://mlresearch.readthedocs.io/en/latest/?badge=latest}{ML-Research}
libraries. All functions, algorithms, experiments and results are provided in
the \href{https://github.com/joaopfonseca/ml-research/}{GitHub repository of
the project}.

\section{Results \& Discussion}~\label{sec:results_discussion}

In a multiple dataset experiment, the analysis of results should not rely
uniquely on the average performance scores across datasets. The domain of
application and fluctuations of performance scores between datasets make the
analysis of these averaged results less accurate. Instead, it is generally
recommended the use of the mean ranking scores to extend the
analysis~\cite{Demsar2006}. Since mean performance scores are still intuitive
to interpret, we will present and discuss both results. The rank values are
assigned based on the mean scores of 3 different runs of 5-fold Cross
Validation (15 performance estimations per dataset) for each combination of
dataset, AL configuration, classifier and performance metric.

\subsection{Results}~\label{sec:results}

The average ranking of the AULC estimations of AL methods are shown in
Table~\ref{tab:aulc_ranks}. The proposed method almost always improves AL
performance and ensures higher data selection efficiency.

% TODO: 
% - Captions need to be rewritten
% - Convert this table into a bar plot?
\begin{table}[H]
    \centering
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/mean_std_aulc_ranks.csv}
    \caption{%
        Mean rankings of the AULC metric over the different datasets (10),
        folds (5) and runs (3) used in the experiment. The proposed method
        always improves the results of the original framework and on average
        almost always improves the results of the oversampling framework.
    }\label{tab:aulc_ranks}
\end{table}

Table~\ref{tab:aulc_scores} shows the average AULC scores, grouped by
classifier, Evaluation Metric and AL framework. The variation in performance
across active learners is consistent with the mean rankings found in
Table~\ref{tab:aulc_ranks}, while showing significant AULC score differences
between the proposed AL method and the oversampling AL method.

\begin{table}[htb]
    \centering
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/mean_std_aulc_scores.csv}
    \caption{\label{tab:aulc_scores}
        Average AULC of each AL configuration tested. Each AULC score is
        calculated using the performance scores of each iteration in the
        validation set. By the end of the iterative process, each AL
        configuration used a total of XXXX\% instances of the XXXX\% instances
        that compose the training sets.
    }
\end{table}

The average DUR scores were calculated for various G-mean thresholds, varying
between 0.1 and 1.0 at a 0.02 step (45 different thresholds in total).
Table~\ref{tab:optimal_data_utilization} shows the results obtained for these
scores starting from a G-mean score of 0.6 and was filtered to show only the
thresholds ending with 0 or 6. In most cases, the proposed method reduces the
amount of data annotation required to reach each G-mean score threshold.

\begin{table}[H]
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/optimal_data_utilization.csv}
    \caption{\label{tab:optimal_data_utilization}
        Mean data utilization of AL algorithms, as a percentage of the
        training set.
    }
\end{table}

The DUR scores relative to the Standard AL method are shown in
Figure~\ref{fig:dur}. A DUR below 1 means that the Proposed/Oversampling
method requires less data than the Standard AL method to reach the same
performance threshold. For example, running an AL strategy using the KNN
classifier requires 69.6\% of the amount of data required by the Standard AL
method using the same classifier to reach an F-Score of 0.62 (\textit{i.e.,}
requires 30.4\% less data).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{../analysis/data_utilization_rate}
    \caption{%
        Mean data utilization rates. The y-axis shows the percentage of data
        (relative to the baseline AL framework) required to reach the
        different performance thresholds.
    }~\label{fig:dur}
\end{figure}

% TODO: Discuss how this technique can be used to train classifiers over fully
% labeled datasets with the sole purpose of increasing their quality.
The mean optimal classification scores of AL methods and Classifiers (fully
labeled training set, without AL) is shown in
Table~\ref{tab:optimal_mean_std_scores}. The proposed AL method produces
classifiers that are almost always able to outperform classifiers using the
full training set (\textit{i.e.,} the ones labeled as MP).

\begin{table}[H]
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/optimal_mean_std_scores.csv}
    \caption{\label{tab:optimal_mean_std_scores}
        Optimal classification scores. The Maximum Performance (MP)
        classification scores are calculated using classifiers trained using
        the entire training set.
    }
\end{table}


% \begin{table}[H]
%     \centering
%     \pgfplotstabletypeset[
%         col sep=comma,
%         string type,
%         every head row/.style={%
%             before row=\toprule,
%             after row=\midrule
%         },
%         every last row/.style={after row=\bottomrule},
%     ]{../analysis/wide_optimal_aulc.csv}
%     \caption{\label{tab:wide_optimal_aulc}
%         AULC scores of each AL configuration tested over the different
%         datasets. Each AULC score is calculated using the G-mean scores of
%         each iteration in the validation set. By the end of the iterative
%         process, each AL configuration used a total of 750 instances of the
%         960 instances that compose the training set.
%     }
% \end{table}


\subsection{Statistical Analysis}~\label{sec:statistical-analysis}

When checking for statistical significance in a multiple dataset context it is
important to account for the multiple comparison problem. Consequently, our
statistical analysis focuses on the recommendations found
in~\cite{Demsar2006}. Overall, we perform 3 statistical tests. The Friedman
test~\cite{Friedman1937} is used to understand whether there is a
statistically significant difference in performance between the 3 AL
frameworks. As post hoc analysis, the Wilcoxon signed-rank
test~\cite{Wilcoxon1945} was used to check for statistical significance
between the performance of the proposed AL method and the oversampling AL
method across datasets. As a second post hoc analysis, the
Holm-Bonferroni~\cite{Holm1979} method was used to check for statistical
significance between the methods using data generators and the Standard AL
framework across classifiers and evaluation metrics.

Table~\ref{tab:friedman_test} contains the \textit{p-values} obtained with the
Friedman test. The difference in performance across AL frameworks is
statistically significant at a level of $\alpha = 0.05$ regardless of the
classifier or evaluation metric being considered.

\begin{table}[H]
	\centering
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/friedman_test.csv}
    \caption{%
        Results for Friedman test. Statistical significance is tested at a
        level of $\alpha = 0.05$. The null hypothesis is that there is no
        difference in the classification outcome across oversamplers.
    }\label{tab:friedman_test}
\end{table}

Table~\ref{tab:wilcoxon_test} contains the \textit{p-values} obtained with the
Wilcoxon signed-rank test. The proposed method was able to outperform both the
standard AL framework, as well as the AL framework using a normal oversampling
strategy proposed in~\cite{Fonseca2021} with statistical significance in 9 out
of 10 datasets.

\begin{table}[H]
	\centering
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/wilcoxon_test.csv}
    \caption{%
        Adjusted p-values using the Wilcoxon signed-rank method. Bold values
        are statistically significant at a level of $\alpha = 0.05$. The null
        hypothesis is that the performance of the proposed framework is
        similar to that of the oversampling or standard framework.
    }\label{tab:wilcoxon_test}
\end{table}

The \textit{p-values} shown in Table~\ref{tab:holms_test} refer to the results
of the Holm-Bonferroni test. The proposed method's superior performance was
statistically significant for any combination of classifier and evaluation
metric. Simultaneously, the proposed method established statistical
significance in the 3 scenarios where the oversampling AL method failed to do
so.

\begin{table}[H]
	\centering
    \pgfplotstabletypeset[
        col sep=comma,
        string type,
        every head row/.style={%
            before row=\toprule,
            after row=\midrule
        },
        every last row/.style={after row=\bottomrule},
    ]{../analysis/holms_test.csv}
    \caption{%
		Adjusted p-values using the Holm-Bonferroni method. Bold values
        are statistically significant at a level of $\alpha = 0.05$. The 
        null hypothesis is that the tested method does not perform better 
        than the control method (benchmark AL framework).
    }\label{tab:holms_test}
\end{table}


% TODO: add analysis of dataset complexity versus AL performance


\subsection{Discussion}~\label{sec:sub_discussion}

In this paper we study the application of data augmentation methods through
the modification of the standard AL framework. This is done to further reduce
the amount of labeled data required to produce a reliable classifier, at the
expense of artificial data generation.

% a different data generation strategy
% - Superiority of AL proposed vs standard (AULC + statistical analysis)
In Table~\ref{tab:aulc_ranks} we found that the proposed method was able to
outperform the Standard AL framework in all scenarios. The mean rankings are
consistent with the mean AULC scores found in Table~\ref{tab:aulc_scores},
while showing significant performance differences between the proposed method
and both the standard and oversampling methods. The Friedman test in
Table~\ref{tab:friedman_test} showed that the difference in the performance of
these AL frameworks is statistically significant, regardless of the
classifier or performance metric being used.

% parameter optimization within the iterative process of an AL procedure
% - Discuss consistency of results as compared with other methods (DUR metric)
The proposed method showed more consistent data utilization requirements to
most of the assessed G-mean score thresholds when compared to the remaining AL
methods, as seen in Table~\ref{tab:optimal_data_utilization}. For example, to
reach a G-mean Score of 0.9 using the KNN and LR classifiers, the average
amount of data required with the Oversampling AL approach increased when
compared to the Standard approach. However, the proposed method was able to
decrease the amount of data required in both situations. The robustness of the
Proposed method is clearer in Figure~\ref{fig:dur}. In most cases, this method
was able outperform the Oversampling method. At the same time, the
proposed method also addresses inconsistencies in situations where the
Oversampling method was unable to outperform the standard method.

% Data augmentation vs oversampling
The statistical analyses found in Tables~\ref{tab:wilcoxon_test}
and~\ref{tab:holms_test} showed that the proposed method's superiority was
statistically significant in all datasets except one (Baseball) and
established statistical significance when compared to the Standard AL method
for all combinations of classifier and performance metric, including when the
Oversampling AL method failed to do so. These results show that the Proposed
method increased the reliability of the new AL framework while improving the
quality of the final classifier while requiring less data.

% Usage of AL as a method to produce better performing classifiers, even in
% settings with fully labeled data
To the best of our knowledge, the method proposed in this paper was the first
AL approach to consistently outperform the maximum performance threshold.
Specifically, in Table~\ref{tab:optimal_mean_std_scores}, the performance of
the classifiers originating from the proposed method was able to outperform
classifiers trained using the full training dataset in all 12 scenarios except
one. This shows that using a meaningful subset of the training dataset along
with data augmentation not only matches the classification performance of ML
algorithms, as it also improves them. Even in a setting with fully labeled
training data, the proposed method may be used as preprocessing method to
further optimize classification performance.

% Future work and limitations
This study introduces data augmentation within the AL framework, along with
the exploration of optimal augmentation methods within AL iterations. However,
the conceptual nature of this study implies some limitations. Specifically,
the large amount of experiments required to test the method's efficacy, along
with the limited computational power available, led to a limited exploration
of the grid search's potential. Future work should focus into understanding
how the usage of a more comprehensive parameter tuning approach improves the
quality of the AL method. In addition, the proposed method was not able to
outperform the standard AL method in 100\% of scenarios. The exploration of
other, more complex, data augmentation techniques might further improve its
performance through the production of more meaningful training observations.
Specifically, in this study we assume that all datasets used follow a
manifold, allowing the usage of G-SMOTE as a data augmentation approach.
However, this method cannot be used into more complex, non-euclidean spaces.
In this scenario, the usage of G-SMOTE is not valid and might lead to the
production of noisy data. Deep Learning-based data augmentation techniques are
able to address this limitation and improve the overall quality of the
artificial data being generated. We also found significant standard errors
throughout our experimental results (see Subsection~\ref{sec:results}), which
is consistent with the findings in~\cite{Fonseca2021, Kottke2017}. This
indicates that the production of more robust generators did not decrease the
standard error of AL performance. Instead, AL's performance variability is
likely dependent on the quality of its initialization.

\section{Conclusion}~\label{sec:conclusion}

\textbf{WIP:\@ THE TEXT BELOW IS A DRAFT (i.e., just some notes I wrote to
remind myself of important topics to discuss later)}

In this study, we introduce two modifications of the AL framework: (1) The
exploration of a different data generation strategy (\textit{i.e.,} data
augmentation using G-SMOTE beyond the typical oversampling strategy) and (2)
Add parameter optimization within the iterative process of an AL procedure.

These modifications were developed with the goal of reducing the amount of
iterations required to produce a classifier with a performance as good as
classifiers trained with the entire training dataset (\textit{i.e.,} without
the need of AL). This is done via the collection of a reduced data subset
containing the most informative observations for the training phase of the
classifier. With our contribution, data selection in AL iterations target
observations that optimize the quality of the artificial data produced. The
substitution of labeled data with artificial data is especially useful in this
context, since it allows the reduction of user interaction necessary to reach
a sufficiently informative dataset.

The AL framework discussed in this study was recently proposed within the Remote
Sensing domain for Land Use/Land Cover classification~\cite{Fonseca2021}. We
further extend this study by applying the AL framework discussed in a context
agnostic environment, considering 10 different multiclass datasets from
different domains.

\bibliography{references}
\bibliographystyle{ieeetr}

\end{document}
