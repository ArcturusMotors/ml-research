\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\usepackage{breakcites}
\usepackage[square,numbers]{natbib}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{%
	a4paper,
	left=18mm,
	right=18mm,
	top=18mm,
}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\definecolor{hypecol}{HTML}{0875b7}
\hypersetup{%
    colorlinks,
    linkcolor={hypecol},
    citecolor={hypecol},
    urlcolor={hypecol}
}

\title{%
    Research Trends and Applications of Data Augmentation Algorithms
}

\author{%
	Joao Fonseca\(^{1*}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de
    Campolide, 1070--312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\begin{document}

\maketitle

\section{Introduction}

Introduction goes here.

\section{Theory}

JÃ¼rgen Schmidhuber's group shows that a simple MLP architecture can achieve
state-of-the-art performance on computer vision benchmarks given strong enough
data augmentation [1,2].

[1] Better digit recognition with a committee of simple Neural Nets. Meier,
Cires, Gambardella and Schmidhuber 2011 [PDF]

[2] Handwritten Digit Recognition with a Committee of DeepNeural Nets on GPUs.
Ciresan, Meier, Gambardella and Schmidhuber 2011 [PDF]

\section{Methodology}

In this section we describe the procedures defined for the literature
collection, data preprocessing and literature analysis. The analysis of the
literature was developed with 3 different approaches. Throughout the
analyses, data preprocessing and hyperparameter tuning was developed
iteratively. The procedure adopted in this manuscript is shown in
Figure~\ref{fig:slr_diagram}.

The literature collection procedure is described in
Subsection~\ref{sec:lit_collection}. The data and text preprocessing is
described in Subsection~\ref{sec:data_preprocessing}. The exploratory data
analysis described in Subsection~\ref{sec:journal_and_conference_analysis} was
done to understand which manuscripts, journals and conferences are most
significant within the field of Data Augmentation. The manuscripts' keywords
were used to construct a network of keywords (described in
Subsection~\ref{sec:keyword_analysis}) and study the different communities of
keywords found in the network. The topic modelling and parameter tunning is described in
Subsection~\ref{sec:topic_modelling}. The abstract embeddings procedure is
described in Subsection~\ref{sec:abstract_embeddings}.

\begin{figure}[H]
	\centering
	\includegraphics[width=.75\linewidth]{../analysis/slr_diagram}
    \caption{Diagram of the proposed literature analysis approach.
    }~\label{fig:slr_diagram}
\end{figure}

\subsection{Literature Collection}~\label{sec:lit_collection}

The focus of this literature analysis is to understand the different
algorithms, domains and/or tasks that employ data augmentation techniques.
Therefore, we use the keyword ``data augmentation'' in order to ensure an
unbiased analysis. The results were then limited to conference papers and
journal articles written in English that were published in the past 15 years.
Due to the large amount of results found, using using solely the
\href{https://www.scopus.com/}{Scopus} database was found to be sufficient.
The resulting query is shown below:

\begin{verbatim}
    KEY ( "data augmentation" )  AND  ( LIMIT-TO ( LANGUAGE ,  "English" ) )  
    AND ( LIMIT-TO ( DOCTYPE ,  "cp" )  OR  LIMIT-TO ( DOCTYPE ,  "ar" ) )  
    AND  (
            LIMIT-TO ( PUBYEAR ,  2021 )  OR  LIMIT-TO ( PUBYEAR ,  2020 )  
        OR  LIMIT-TO ( PUBYEAR ,  2019 )  OR  LIMIT-TO ( PUBYEAR ,  2018 )  
        OR  LIMIT-TO ( PUBYEAR ,  2017 )  OR  LIMIT-TO ( PUBYEAR ,  2016 )  
        OR  LIMIT-TO ( PUBYEAR ,  2015 )  OR  LIMIT-TO ( PUBYEAR ,  2014 )  
        OR  LIMIT-TO ( PUBYEAR ,  2013 )  OR  LIMIT-TO ( PUBYEAR ,  2012 )  
        OR  LIMIT-TO ( PUBYEAR ,  2011 )  OR  LIMIT-TO ( PUBYEAR ,  2010 )  
        OR  LIMIT-TO ( PUBYEAR ,  2009 )  OR  LIMIT-TO ( PUBYEAR ,  2008 )  
        OR  LIMIT-TO ( PUBYEAR ,  2007 )  OR  LIMIT-TO ( PUBYEAR ,  2006 ) 
    )  
\end{verbatim}

The resulting data selection/filtering pipeline is shown in
Figure~\ref{fig:data_filtering_pipeline}. Due to the limitations in the Scopus
data export (maximum 2000 documents per export), the data was split in four
different time periods and exported separately: 2006 until 2018, 2019, 2020
and 2021, which produced four CSV files.

\subsection{Data Preprocessing}~\label{sec:data_preprocessing}

The each step of the data preprocessing stage and amount of documents dropped
is represented in Figure~\ref{fig:data_filtering_pipeline}. The data was first
concatenated into a single data frame. During this process, we found that one
of the exported references had a corrupted line, which caused the loss of one
additional document. References without a DOI were disregarded from further
analysis. Since they are used as unique identifiers for intellectual
property~\cite{Paskin1999}, we used this variable to detect and remove
duplicate references from the dataset.

Removed references without a single citation, 2259 results. Exception made for
journals and conference analysis.  

For the network analysis:

Out of the 2259 results, 1923 contained keywords in Scopus' database.

Keyword combinations showing up in only one document are removed from further
analysis.


\begin{figure}[H]
	\centering
    \includegraphics[width=.55\linewidth]{../analysis/data_filtering_pipeline}
    \caption{Data filtering pipeline.
    }~\label{fig:data_filtering_pipeline}
\end{figure}


The network consists of an undirected graph whose weights consist of the
following formula: $Keyword--pair weight = \log(Avg cites * Nbr of documents)$
to avoid a disproportional bias of highly cited research articles.

\begin{itemize}
    \item Discuss data preprocessing done.
    \item Concatenate data, extract features (if possible), data cleaning etc
\end{itemize}

\subsection{Journal and Conference analysis}~\label{sec:journal_and_conference_analysis}

\subsection{Keyword Analysis}~\label{sec:keyword_analysis}

\subsection{Topic Modelling}~\label{sec:topic_modelling}

\subsection{Abstract embeddings}~\label{sec:abstract_embeddings}

\subsection{Software Implementation}~\label{sec:software_implementation}

The analysis and modelling was developed using the Python programming
language, along with the
\href{https://scikit-learn.org/stable/}{Scikit-Learn}~\cite{Pedregosa2011},
\href{https://radimrehurek.com/gensim/}{Gensim}~\cite{Rehurek2010},
\href{https://github.com/lmcinnes/umap}{Umap-Learn}~\cite{Mcinnes2018} and
\href{https://networkx.org/}{Networkx}~\cite{Hagberg2008} libraries. The final
network analysis and visualization was done with
\href{https://gephi.org/}{Gephi}~\cite{Bastian2009}. All functions,
algorithms, analyses and results are provided in the
\href{https://github.com/joaopfonseca/research}{GitHub repository of the
project}.








\section{Results}

Results go here.

\subsection{PRISMA Flow Diagram}

\begin{itemize}
    \item Create a flowchart with the data cleaning process and describe it.
\end{itemize}

\subsection{Terms Frequency}

\subsection{Topics Discovered}

LDA analysis goes here

\subsubsection{Main Journals}

\subsubsection{Main Conference Proceedings}

\subsection{Author Co-occurrence Analysis}

Not sure whether to keep this one.

\subsection{Title and Abstract Text Occurrence Analysis}

This can be done by text occurrence network visualization or embedings

\subsection{Most Cited Publications}

\subsection{Application and Method Analysis}

\section{Discussion}

Discussion goes here.

\subsection{Research Question Discussion}

\subsection{Research Gap Discussion}

\subsection{Study Limitation Discussion}

\section{Conclusions}

\bibliography{references}
% \bibliographystyle{apalike}
\bibliographystyle{ieeetr}

\end{document}
